# -*- coding: utf-8 -*-
"""hw_5_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14NegWKiCnlNvIb1kwqxAh6XNNYeVdcDj

# Homework 5 Coding Part 1 - DQN

## CartPole Problem (Deep Q-learning)

In this assignment, we will solve the "CartPole-v1" task from the OpenAI Gym using the Deep Q-Network (DQN) algorithm.

### Problem Desciption

A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over.


![image.png](data:image/png;base64,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)




### Formulation

- *State* $s$:
    The state consists of four observations as follows:

|Num|Observation|Min|Max|
|:-:|:-:|:-:|:-:|
|0|Cart Position|-4.8|4.8|
|1|Cart Velocity|-Inf|Inf|
|2|Pole Angle|-0.418 rad(-24 deg)|0.418 rad (24 deg)|
|3|Pole Angular Velocity|-Inf|Inf|

All observations of the initial state are assigned a uniform random value in [-0.05,0.05].


- *Action $a$*:
    There are two possible actions as follows:

|Num|Action|
|:-:|:-:|
|0|Push cart to the left|
|1|Push cart to the right|


- *Reward $r(s,a)$*:
    
    Reward is 1 for every step taken, including the termination step.
    

- Episode Termination

    An episode terminates when one of the following occurs:
  - Pole Angle is more than 12 degrees.
  - Cart Position is more than 2.4 (Cart reaches the edge of the display).
  - Episode length is greater than 500.


- *Objective*:

    The CarPole problem is considered solved when the average return is greater than or equal to 475.0 over 100 consecutive trials (episodes).
"""

# Import packages. Run this cell.

import numpy as np
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import random

"""### Tasks

**DQN** (20 points)

We will use DQN algorithm to solve the CartPole problem.

Please complete functions  ``select_action(self, state)`` and  ``train(self, s0, a0, r, s1, done)`` in the class ``DQN``.
   - ``select_action(self, state)`` will be called each step and returns an action for the agent to take.
   - ``train(self, s0, a0, r, s1, done)`` will be called each step after the agent takes an action ``a0`` in the current state ``s0``, receives a reward ``r``, and observes the next state ``s1``. ``done=True`` means that the episode terminates and ``done=False`` means that the episode does not terminate.
   
You can also add or revise classes and functions if you need.

**Note**: The platform does not support GPU, so please use CPU. A small neural network is enough for this problem. Using a large neural network will take a long time to train and evaluate.

**Recommended Hyperparameters**:
- Discount factor: 0.99
- Batch size: 64
- Replay buffer: 10000
- Epsilon max : 1.0
- Epsilon min: 0.01
- Epsilon decay: 0.95 (per episode)
- Optimizer: Adam
- Learning rate: 0.0005
- For the Q network, you can use the class `QNetwork` with `hidden_dim=128`
- Update target network every one episode
- Loss function: MSE
"""

"""
A simple Q-network class
"""
class QNetwork(torch.nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim):
        """
        Args:
            input_dim (int): state dimension.
            output_dim (int): number of actions.
            hidden_dim (int): hidden layer dimension (fully connected layer)
        """
        super().__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, state):
        """
        Returns a Q value
        Args:
            state (torch.Tensor): state, 2-D tensor of shape (n, input_dim)
        Returns:
            torch.Tensor: Q values, 2-D tensor of shape (n, output_dim)
        """
        x = F.relu(self.linear1(state))
        x = self.linear2(x)
        return x


"""
Agent class that implements the DQN algorithm
"""
class DQN:
    def __init__(self, seed=None):
        self.output_dim = 2  # Output dimension of Q network, i.e., the number of possible actions
        self.dqn = QNetwork(4, self.output_dim, 128)  # Q network
        self.dqn_target = QNetwork(4, self.output_dim, 128)  # Target Q network
        self.dqn_target.load_state_dict(self.dqn.state_dict())
        self.batch_size = 64  # Batch size
        self.gamma = 0.99  # Discount factor
        self.eps = 1.0  # epsilon-greedy for exploration
        self.loss_fn = torch.nn.MSELoss()  # loss function
        self.optim = torch.optim.Adam(self.dqn.parameters(), lr=5e-4)  # optimizer for training
        self.replay_memory_buffer = deque(maxlen=10000)  # replay buffer
        if seed is None:
            self.rng = np.random.default_rng()
        else:
            self.rng = np.random.default_rng(seed)

    def select_action(self, state):
        """
        Returns an action for the agent to take during training process
        Args:
            state: a numpy array with size 4
        Returns:
            action: an integer, 0 or 1
        """

        # Please complete codes for choosing an action given the current state
        """
        Hint: You may use epsilon-greedy for exploration.
        With probability self.eps, choose an action uniformly at random;
        Otherwise, choose a greedy action based on the output of the Q network (self.dqn).
        """
        ### BEGIN SOLUTION
        # YOUR CODE HERE
        if self.rng.random()<self.eps:
            action=self.rng.integers(0,self.output_dim)
        else:
            state=torch.from_numpy(state).float().unsqueeze(0)
            with torch.no_grad():
                q_values=self.dqn(state)
                action=q_values.max(1)[1].item()
        # raise NotImplementedError()
        ### END SOLUTION
        return action

    def train(self, s0, a0, r, s1, done):
        """
        Train the Q network
        Args:
            s0: current state, a numpy array with size 4
            a0: current action, 0 or 1
            r: reward
            s1: next state, a numpy array with size 4
            done: done=True means that the episode terminates and done=False means that the episode does not terminate.
        """
        self.add_to_replay_memory(s0, a0, r, s1, done)

        if done:
            self.update_epsilon()
            self.target_update()

        if len(self.replay_memory_buffer) < self.batch_size:
            return

        """
        state_batch: torch.Tensor with shape (self.batch_size, 4), a mini-batch of current states
        action_batch: torch.Tensor with shape (self.batch_size, 1), a mini-batch of current actions
        reward_batch: torch.Tensor with shape (self.batch_size, 1), a mini-batch of rewards
        next_state_batch: torch.Tensor with shape (self.batch_size, 4), a mini-batch of next states
        done_list: torch.Tensor with shape (self.batch_size, 1), a mini-batch of 0-1 integers,
                   where 1 means the episode terminates for that sample;
                         0 means the episode does not terminate for that sample.
        """
        mini_batch = self.get_random_sample_from_replay_mem()
        state_batch = torch.from_numpy(np.vstack([i[0] for i in mini_batch])).float()
        action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).int()
        reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float()
        next_state_batch = torch.from_numpy(np.vstack([i[3] for i in mini_batch])).float()
        done_list = torch.from_numpy(np.vstack([i[4] for i in mini_batch]).astype(np.uint8)).float()

        # Please complete codes for updating the Q network self.dqn
        """
        Hint: You may use the above tensors: state_batch, action_batch, reward_batch, next_state_batch, done_list
              You may use self.dqn_target as your target Q network
              You may use self.loss_fn (or torch.nn.MSELoss()) as your loss function
              You may use self.optim as your optimizer for training the Q network
        """
        ### BEGIN SOLUTION
        # YOUR CODE HERE
        current_q_value=self.dqn(state_batch).gather(1,action_batch)
        next_q_value=self.dqn_target(next_state_batch).max(1)[0].unsqueeze(1)
        target_q_value = reward_batch + (self.gamma * next_q_value * (1 - done_list))
        loss = self.loss_fn(current_q_value, target_q_value)
        self.optim.zero_grad()
        loss.backward()
        self.optim.step()
        #raise NotImplementedError()
        ### END SOLUTION
        return

    def add_to_replay_memory(self, state, action, reward, next_state, done):
        """
        Add samples to replay memory
        Args:
            state: current state, a numpy array with size 4
            action: current action, 0 or 1
            reward: reward
            next_state: next state, a numpy array with size 4
            done: done=True means that the episode terminates and done=False means that the episode does not terminate.
        """
        self.replay_memory_buffer.append((state, action, reward, next_state, done))

    def get_random_sample_from_replay_mem(self):
        """
        Random samples from replay memory without replacement
        Returns a self.batch_size length list of unique elements chosen from the replay buffer.
        Returns:
            random_sample: a list with len=self.batch_size,
                           where each element is a tuple (state, action, reward, next_state, done)
        """
        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)
        return random_sample

    def update_epsilon(self):
        # Decay epsilon
        if self.eps >= 0.01:
            self.eps *= 0.95

    def target_update(self):
        # Update the target Q network (self.dqn_target) using the original Q network (self.dqn)
        self.dqn_target.load_state_dict(self.dqn.state_dict())

# You may use this cell for debugging. You can delete this cell before submitting your code to save run time.
env = gym.make('CartPole-v1')
env.seed(0)
random.seed(0)
np.random.seed(0)
torch.manual_seed(0)
agent = DQN(seed=0)
env.reset()
for i in range(1):
    state = env.reset()
    done = False
    episodic_reward = 0
    while not done:
        action = agent.select_action(np.squeeze(state))
        next_state, reward, done, info = env.step(action)
        episodic_reward += reward
        agent.train(state, action, reward, next_state, done)
        state = next_state
    print(f'Episode {i + 1}, reward: {episodic_reward}')
env.close()